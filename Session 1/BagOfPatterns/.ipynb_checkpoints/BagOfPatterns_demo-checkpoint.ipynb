{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "execute:\n",
    "    echo: false\n",
    "    warning: false\n",
    "    \n",
    "format:\n",
    "    html:\n",
    "        theme: \"theme_ipol.scss\"\n",
    "        page-layout: full\n",
    "        self-contained: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "SRCDIR = \".\"\n",
    "\n",
    "w = 100\n",
    "a = 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation-invariant similarity in time series using bag-of-patterns representation\n",
    "\n",
    "Implementation of Lin et. al. (2012), a histogram-based representation for time series data, similar to the “bag of words” approach - called \"bag of patterns\".\n",
    "\n",
    "_Inputs:_\n",
    "- _S: list of  time series as a numpy arrays_\n",
    "- _n: size of the sliding window_\n",
    "- _w: word length - also length of the extracted patterns_\n",
    "- _a: alphabet size_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from scipy.stats import rankdata, norm\n",
    "from statsmodels.api import qqplot\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(SRCDIR)\n",
    "\n",
    "# Plot parameters\n",
    "plt.rc('figure', titlesize=22)\n",
    "plt.rc('axes', edgecolor='gray', facecolor='#FAFAFA', \n",
    "       titlecolor='#242424', titlesize=22, titlepad=10,\n",
    "       labelcolor='#242424', labelsize=15)\n",
    "plt.rc('grid', color='grey', linestyle='dotted', linewidth=0.5, alpha=0.8)\n",
    "plt.rc('xtick', direction='out', color='#242424', labelsize=15)\n",
    "plt.rc('ytick', direction='out', color='#242424', labelsize=15)\n",
    "plt.rc('lines', linewidth=1)\n",
    "plt.rc('figure.subplot', hspace=0.4)\n",
    "\n",
    "# Pandas DataFrame parameters\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.panel-tabset}\n",
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string(column): \n",
    "    \"\"\"\n",
    "    Converts a list of strings into a string. E.g. ['a', 'b', 'c'] --> 'abc'\n",
    "    \"\"\"\n",
    "    return ''.join(l for l in column)\n",
    "\n",
    "def motifs_table(S_discrete_list):\n",
    "    \"\"\"\n",
    "    Given a list of discrete subsequences, store their starting position and their frequency\n",
    "    \"\"\"\n",
    "    table = defaultdict(list)\n",
    "    freq = defaultdict(int)\n",
    "    \n",
    "    for i, seq in enumerate(S_discrete_list):\n",
    "        word = make_sentence(seq)\n",
    "        table[word].append(i)\n",
    "        freq[word] += 1\n",
    "    \n",
    "    return table, freq\n",
    "\n",
    "def extract_patterns(S, n):\n",
    "    \"\"\"\n",
    "    Given a time series S as a numpy array, extract all subsequences, of length n, with a stride of 1.\n",
    "    Returns a numpy array, each row corresponding to a subsequence. \n",
    "    \n",
    "    Uses an indexer matrix for faster computation time.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(S)\n",
    "    \n",
    "    # k x n matrix with the correct indices to extract the subsequences\n",
    "    window_indexer = np.array(\n",
    "        np.expand_dims(np.arange(n), 0) + \n",
    "        np.expand_dims(np.arange(N-n), 0).T\n",
    "    )\n",
    "    \n",
    "    return S[window_indexer]\n",
    "\n",
    "def plot_bop(S, M_all):    \n",
    "    \"\"\"\n",
    "    Simple plot function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract important parameters from the arrays\n",
    "        # Merge all keys from the dictionaries in M_all, and reset their values to 0\n",
    "    merged_dict = {}\n",
    "    for m in M_all:\n",
    "        merged_dict.update(m)\n",
    "    global_dictionary = {key: 0 for key in merged_dict} \n",
    "        # Get the max values of the time series\n",
    "    max_len = 0\n",
    "    max_val = 2\n",
    "    min_val = -2\n",
    "    for s, m in zip(S, M_all):\n",
    "        s = zscore_norm(s)\n",
    "        if len(s) > max_len:\n",
    "            max_len = len(s)\n",
    "        if max(s) > max_val:\n",
    "            max_val = max(s)\n",
    "        if min(s) < min_val:\n",
    "            min_val = min(s)\n",
    "        \n",
    "    # Plot \n",
    "    # f, axes = plt.subplots(len(S), 2, figsize=(15, len(S)*2))\n",
    "    f = plt.figure(figsize=(15, len(S)*2))\n",
    "    gs = f.add_gridspec(len(S), 3)\n",
    "    colors = sns.color_palette(\"hls\", 8)\n",
    "    \n",
    "    #for i, (s, m, ax1, ax2) in enumerate(zip(S, M_all, axes.ravel()[::2], axes.ravel()[1::2])):\n",
    "    for i, (s, m) in enumerate(zip(S, M_all)):\n",
    "        # Normalized time series\n",
    "        ax1 = f.add_subplot(gs[i, 0])\n",
    "        ax1.plot(zscore_norm(s), c=colors[i%len(colors)])\n",
    "        #ax1.set_xlim((0, max_len))\n",
    "        ax1.set_xlim((0, len(s)))\n",
    "        ax1.set_ylim((min_val, max_val))\n",
    "        # Histogram\n",
    "        hist = {**global_dictionary , **m} # Merge dict so every histogram has the same x-axis\n",
    "        keys = np.sort(list(hist.keys())) # Sort keys by alphabetical order\n",
    "        vals = [list(hist.values())[i]/max(list(hist.values())) for i in np.argsort(list(hist.keys()))]\n",
    "        ax2 = f.add_subplot(gs[i, 1:])\n",
    "        ax2.bar(keys, vals, color=colors[i%len(colors)])\n",
    "        ax2.set_ylim((0, 1.1))\n",
    "        ax2.tick_params(axis=\"x\", rotation=90, labelsize=12)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.panel-tabset}\n",
    "# SAX Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization: INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_INT(df, df_INT):\n",
    "    \n",
    "    for col in df.columns[1:]:\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(12, 2)) \n",
    "\n",
    "        ax.plot(df_INT['t'], df_INT[col])\n",
    "        ax.set_title(f'INT - Column {col}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(f'{col}')\n",
    "        plt.show()\n",
    "\n",
    "        f = plt.figure(figsize=(12, 2))\n",
    "        ax = f.add_subplot(1, 2, 1)\n",
    "        qqplot(df[col], line='45', ax=ax, c='royalblue', markersize=0.5)\n",
    "        ax.set_xlabel('Normal Theoretical Quantiles')\n",
    "        ax.set_ylabel('Data Quantiles')\n",
    "        ax.set_title(f'QQ-plot - Original data\\nColumn {col}')\n",
    "\n",
    "        ax = f.add_subplot(1, 2, 2)\n",
    "        qqplot(df_INT[col], line='45', ax=ax, c='royalblue', markersize=0.5)\n",
    "        ax.set_xlabel('Normal Theoretical Quantiles')\n",
    "        ax.set_ylabel('Data Quantiles')\n",
    "        ax.set_title(f'QQ-plot - Normalized data\\nColumn {col}')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "def INT(S, c):\n",
    "    \"\"\"\n",
    "    Rank-based inverse normal transformation is a nonparametric transformation to \n",
    "    convert a sample distribution to the normal distribution.\n",
    "    NaN values are ignored.\n",
    "\n",
    "    Inputs:\n",
    "    - S = pandas series \n",
    "    - c = Bloms constant\n",
    "\n",
    "    Output:\n",
    "    - transformed = normally distributed pandas series \n",
    "    \"\"\"\n",
    "\n",
    "    # Get rank, ties are averaged\n",
    "    rank = rankdata(S, method=\"average\")\n",
    "\n",
    "    # Convert rank to normal distribution\n",
    "    n = len(rank)\n",
    "    transformed = norm.ppf((rank-c)/(n-2*c+1))\n",
    "\n",
    "    return transformed\n",
    "\n",
    "def INT_step(df, c=3.0/8):\n",
    "    \"\"\"\n",
    "    Transforms all series in a dataframe using the rank-based inverse normal\n",
    "    transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize output DataFrame\n",
    "    df_INT = pd.DataFrame(index=df.index)\n",
    "    df_INT['t'] = df['t']\n",
    "\n",
    "    # Process each column of the input dataframe\n",
    "    for column in df.columns[1:]:\n",
    "        df_INT[column] = INT(df[column], c)\n",
    "    \n",
    "    # Plot\n",
    "    plot_INT(df, df_INT)\n",
    "\n",
    "    return df_INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_INT = INT_step(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAA & SAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PAA_step(df_INT, w):\n",
    "    \"\"\"\n",
    "    Piecewise Aggregate Approximation (PAA) (Keogh et. al., 2001) for multivariate \n",
    "    time series.\n",
    "\n",
    "    Inputs:\n",
    "    - df_INT = Resulting DataFrame from the normalization step \n",
    "    - w = number of segments\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(df_INT)\n",
    "\n",
    "    # Resample dataframe\n",
    "    new_index = df_INT.index.repeat(w)\n",
    "    df_temp = df_INT.iloc[:, 1:].reindex(new_index) # Do not apply on the time column\n",
    "\n",
    "    # Create new re-indexed dataframe from previously re-indexed dataframe\n",
    "    df_PAA = pd.DataFrame(data=df_temp.to_numpy(),\n",
    "                          index=[i for i in range(w) for k in range(n)],\n",
    "                          columns=df_INT.columns[1:])\n",
    "    \n",
    "    # Average segments\n",
    "    df_PAA = df_PAA.groupby(df_PAA.index).mean()\n",
    "    \n",
    "    # Create a new correct time column\n",
    "    new_t = pd.date_range(start=df_INT['t'].iloc[0], \n",
    "                          end=df_INT['t'].iloc[-1],\n",
    "                          periods=w+1)\n",
    "    df_PAA['t'] = new_t[:-1]\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = df_PAA.columns.tolist()\n",
    "    new_cols = cols[-1:] + cols[:-1]\n",
    "    df_PAA = df_PAA[new_cols]\n",
    "\n",
    "    return df_PAA\n",
    "\n",
    "\n",
    "def plot_SAX_plt(df_INT, df_PAA, df_SAX, breakpoints, w):\n",
    "    \n",
    "    for col in df.columns[1:]:\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(12, 2))\n",
    "\n",
    "        for line in breakpoints:\n",
    "            ax.axhline(line, c='crimson', alpha=0.4, ls='--', lw=0.7)\n",
    "\n",
    "        dt = df_SAX['t'].iloc[1] - df_SAX['t'].iloc[0]\n",
    "        for t, letter in enumerate(df_SAX[col]):\n",
    "            ax.text(df_PAA['t'].iloc[t]+dt/2, \n",
    "                    df_PAA[col].iloc[t]+0.1, \n",
    "                    f'{letter}', c='crimson', \n",
    "                    ha='center', fontsize=15)\n",
    "\n",
    "        # PAA\n",
    "        ax.step(df_PAA['t'], df_PAA[col], where='post', c='royalblue', alpha=0.9)\n",
    "        ax.plot([df_PAA['t'].iloc[-1], df_INT['t'].iloc[-1]], \n",
    "                [df_PAA[col].iloc[-1], df_PAA[col].iloc[-1]], \n",
    "                c='royalblue', alpha=0.9)\n",
    "\n",
    "        # INT Time Series\n",
    "        ax.plot(df_INT['t'], df_INT[col], alpha=0.5)\n",
    "\n",
    "        ax.set_title(f'SAX Representation - w = {w}\\nColumn {col}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_xlim((df_INT['t'].iloc[0], df_INT['t'].iloc[-1]))\n",
    "        ax.set_ylabel(f'{col}')\n",
    "        plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "def distance_table(a, breakpoints):\n",
    "    \"\"\"\n",
    "    Look-up table to compute the distance on symbolized time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    table = np.zeros((a, a))\n",
    "    \n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if abs(i-j) <= 1:\n",
    "                table[i, j] = 0\n",
    "            else:\n",
    "                table[i, j] = breakpoints[max(i, j)] - breakpoints[min(i, j)+1]\n",
    "    \n",
    "    return table\n",
    "\n",
    "def SAX_step(df_PAA, a):\n",
    "    \"\"\"\n",
    "    Symbolic representation of S_paa.\n",
    "\n",
    "    Inputs:\n",
    "    - df_PAA = PAA of a pandas dataframe\n",
    "    - a = alphabet size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define alphabet from alphabet size and corresponding breakpoints\n",
    "    alphabet = np.array([chr(i) for i in range(97, 97 + a)])\n",
    "    breakpoints = norm.ppf(np.linspace(0, 1, a+1)[1:-1])\n",
    "    \n",
    "    # Compute a look-up table that allows us to define a distance\n",
    "    # metric on symbolized time series. Useful for later\n",
    "    dist_table = distance_table(a, breakpoints)\n",
    "    \n",
    "    # Initialize output DataFrame\n",
    "    df_SAX = pd.DataFrame(index=df_PAA.index)\n",
    "    df_SAX['t'] = df_PAA['t']\n",
    "    \n",
    "    # Process each column of the input dataframe\n",
    "    n_timestamps = len(df_PAA)\n",
    "    for column in df.columns[1:]:\n",
    "        \n",
    "        # Convert each value into a letter\n",
    "        discretization = []\n",
    "        for t in range(n_timestamps):\n",
    "            \n",
    "            if np.isnan(df_PAA[column].iloc[t]):\n",
    "                discretization.append(np.nan)\n",
    "            else:\n",
    "                discretization.append(alphabet[np.searchsorted(breakpoints,\n",
    "                                                               df_PAA[column].iloc[t],\n",
    "                                                               side='left')]) \n",
    "    \n",
    "        df_SAX[column] = discretization\n",
    "    \n",
    "    # Plot\n",
    "    plot_SAX_plt(df_INT, df_PAA, df_SAX, breakpoints, w)\n",
    "    \n",
    "    return df_SAX, dist_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PAA = PAA_step(df_INT, w=w)\n",
    "df_SAX, dist_table = SAX_step(df_PAA, a=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.panel-tabset}\n",
    "# BOP Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance function on symbolized time series, according to Lin et. al. (2007)\n",
    "\n",
    "def dist(S_discrete_1, S_discrete_2, dist_table):\n",
    "    \n",
    "    distance = np.zeros(len(S_discrete_1))\n",
    "    \n",
    "    for i, (s1, s2) in enumerate(zip(S_discrete_1, S_discrete_2)):\n",
    "        distance[i] = dist_table[ord(s1)-ord('a'), ord(s2)-ord('a')]\n",
    "        \n",
    "    return np.sqrt(np.sum(distance**2))\n",
    "\n",
    "def min_dist(S_1, S_2, dist_table):\n",
    "    \"\"\"\n",
    "    Given two symbolic representations of time series S_1 and S_2 \n",
    "    as lists of the same length w, computes their distance (Lin et. al. (2007)).\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(S_1)\n",
    "    prefactor = np.sqrt(n/w)\n",
    "    distance = dist(S_1, S_2, dist_table)\n",
    "    \n",
    "    return prefactor*distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOP(S, n, w, a, numerosity_reduction=True, plot=True):\n",
    "    \"\"\"\n",
    "    Implementation of Lin et. al. (2012), a histogram-based representation for time\n",
    "    series data, similar to the “bag of words” approach - called \"bag of patterns\"\n",
    "    \n",
    "        - S: list of  time series as a numpy arrays\n",
    "        - n: size of the sliding window\n",
    "        - w: word length - also length of the extracted patterns\n",
    "        - a: alphabet size\n",
    "    \"\"\"\n",
    "    \n",
    "    S_norm_all = []\n",
    "    S_paa_all = []\n",
    "    S_discrete_all = []\n",
    "    hash_table_all = []\n",
    "    M_all = []\n",
    "        \n",
    "    for s in S:\n",
    "        subsequences = extract_patterns(s, n)\n",
    "\n",
    "        S_norm_list = []\n",
    "        S_paa_list = []\n",
    "        S_discrete_list = []\n",
    "\n",
    "        for seq in subsequences:\n",
    "            S_norm = zscore_norm(seq)\n",
    "            S_paa = paa(S_norm, w)\n",
    "            S_discrete, breakpoints = discretization(S_paa, a)\n",
    "\n",
    "            # Option for numerosity reduction - recommended for slow moving time series\n",
    "            if numerosity_reduction:\n",
    "                # If we have a succession of the same word, we only count it once\n",
    "                if len(S_discrete_list):\n",
    "                    word = ''.join(l for l in S_discrete)\n",
    "                    if word == S_discrete_list[-1]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        S_norm_list.append(S_norm)\n",
    "                        S_paa_list.append(S_paa)\n",
    "                        S_discrete_list.append(''.join(l for l in S_discrete)) \n",
    "                \n",
    "                # If it is the first subsequence, add everything to the arrays\n",
    "                else:\n",
    "                    S_norm_list.append(S_norm)\n",
    "                    S_paa_list.append(S_paa)\n",
    "                    S_discrete_list.append(''.join(l for l in S_discrete)) \n",
    "\n",
    "            # No numerosity reduction\n",
    "            else:\n",
    "                S_norm_list.append(S_norm)\n",
    "                S_paa_list.append(S_paa)\n",
    "                S_discrete_list.append(''.join(l for l in S_discrete))\n",
    "\n",
    "        S_norm = np.concatenate(S_norm_list, axis=None)\n",
    "        S_paa = np.concatenate(S_paa_list, axis=None)\n",
    "        S_discrete = np.concatenate(S_discrete_list, axis=None)\n",
    "\n",
    "        hash_table, M = motifs_table(S_discrete)\n",
    "\n",
    "        S_norm_all.append(S_norm)\n",
    "        S_paa_all.append(S_paa)\n",
    "        S_discrete_all.append(S_discrete)\n",
    "        hash_table_all.append(hash_table)\n",
    "        M_all.append(M)\n",
    "        \n",
    "    if plot:\n",
    "        plot_bop(S, M_all)\n",
    "        \n",
    "    return S_norm_all, S_paa_all, S_discrete_all, hash_table_all, M_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bop_distance(M_all, plot=True):\n",
    "    \"\"\"\n",
    "    The distance is defined as the Euclidian distance between the histograms, i.e. the \n",
    "    Euclidian distance between the pattern's occurrence frequency. \n",
    "    \"\"\"\n",
    "    \n",
    "    distance_matrix = np.zeros((len(M_all), len(M_all)))\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        for j in range(i+1, distance_matrix.shape[1]):\n",
    "            dist = 0\n",
    "            for key in M_all[i].keys():\n",
    "                dist += (M_all[i][key]-M_all[j][key])**2\n",
    "            dist= round(np.sqrt(dist), 2)\n",
    "            distance_matrix[j, i] += dist\n",
    "            \n",
    "    if plot:\n",
    "        f, ax = plt.subplots(figsize=(8, 8))\n",
    "        mask = np.zeros_like(distance_matrix)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        sns.heatmap(distance_matrix, mask=mask, annot=True, fmt='.1f',\n",
    "                    square=True, cbar_kws={'shrink': 0.5}, cmap='Blues', ax=ax)\n",
    "        plt.show()\n",
    "        \n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
