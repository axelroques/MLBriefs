{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation-invariant similarity in time series using bag-of-patterns representation\n",
    "\n",
    "Implementation of Lin et. al. (2012), a histogram-based representation for time series data, similar to the “bag of words” approach - called \"bag of patterns\".\n",
    "\n",
    "_Inputs:_\n",
    "- _S: list of  time series as a numpy arrays_\n",
    "- _n: size of the sliding window_\n",
    "- _w: word length - also length of the extracted patterns_\n",
    "- _a: alphabet size_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(letter_list): \n",
    "    \"\"\"\n",
    "    Converts a list of strings into a string.\n",
    "    E.g.: ['a', 'b', 'c'] --> 'abc'\n",
    "    \"\"\"\n",
    "    return ''.join(l for l in letter_list)\n",
    "\n",
    "def motifs_table(S_discrete_list):\n",
    "    \"\"\"\n",
    "    Given a list of discrete subsequences, store their starting position and their frequency\n",
    "    \"\"\"\n",
    "    table = defaultdict(list)\n",
    "    freq = defaultdict(int)\n",
    "    \n",
    "    for i, seq in enumerate(S_discrete_list):\n",
    "        word = make_sentence(seq)\n",
    "        table[word].append(i)\n",
    "        freq[word] += 1\n",
    "    \n",
    "    return table, freq\n",
    "\n",
    "def extract_patterns(S, n):\n",
    "    \"\"\"\n",
    "    Given a time series S as a numpy array, extract all subsequences, of length n, with a stride of 1.\n",
    "    Returns a numpy array, each row corresponding to a subsequence. \n",
    "    \n",
    "    Uses an indexer matrix for faster computation time.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(S)\n",
    "    \n",
    "    # k x n matrix with the correct indices to extract the subsequences\n",
    "    window_indexer = np.array(\n",
    "        np.expand_dims(np.arange(n), 0) + \n",
    "        np.expand_dims(np.arange(N-n), 0).T\n",
    "    )\n",
    "    \n",
    "    return S[window_indexer]\n",
    "\n",
    "def plot_bop(S, M_all):    \n",
    "    \"\"\"\n",
    "    Simple plot function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract important parameters from the arrays\n",
    "        # Merge all keys from the dictionaries in M_all, and reset their values to 0\n",
    "    merged_dict = {}\n",
    "    for m in M_all:\n",
    "        merged_dict.update(m)\n",
    "    global_dictionary = {key: 0 for key in merged_dict} \n",
    "        # Get the max values of the time series\n",
    "    max_len = 0\n",
    "    max_val = 2\n",
    "    min_val = -2\n",
    "    for s, m in zip(S, M_all):\n",
    "        s = zscore_norm(s)\n",
    "        if len(s) > max_len:\n",
    "            max_len = len(s)\n",
    "        if max(s) > max_val:\n",
    "            max_val = max(s)\n",
    "        if min(s) < min_val:\n",
    "            min_val = min(s)\n",
    "        \n",
    "    # Plot \n",
    "    # f, axes = plt.subplots(len(S), 2, figsize=(15, len(S)*2))\n",
    "    f = plt.figure(figsize=(15, len(S)*2))\n",
    "    gs = f.add_gridspec(len(S), 3)\n",
    "    colors = sns.color_palette(\"hls\", 8)\n",
    "    \n",
    "    #for i, (s, m, ax1, ax2) in enumerate(zip(S, M_all, axes.ravel()[::2], axes.ravel()[1::2])):\n",
    "    for i, (s, m) in enumerate(zip(S, M_all)):\n",
    "        # Normalized time series\n",
    "        ax1 = f.add_subplot(gs[i, 0])\n",
    "        ax1.plot(zscore_norm(s), c=colors[i%len(colors)])\n",
    "        #ax1.set_xlim((0, max_len))\n",
    "        ax1.set_xlim((0, len(s)))\n",
    "        ax1.set_ylim((min_val, max_val))\n",
    "        # Histogram\n",
    "        hist = {**global_dictionary , **m} # Merge dict so every histogram has the same x-axis\n",
    "        keys = np.sort(list(hist.keys())) # Sort keys by alphabetical order\n",
    "        vals = [list(hist.values())[i]/max(list(hist.values())) for i in np.argsort(list(hist.keys()))]\n",
    "        ax2 = f.add_subplot(gs[i, 1:])\n",
    "        ax2.bar(keys, vals, color=colors[i%len(colors)])\n",
    "        ax2.set_ylim((0, 1.1))\n",
    "        ax2.tick_params(axis=\"x\", rotation=90, labelsize=12)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAX Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_norm(S):\n",
    "    \"\"\"\n",
    "    z-normalization of a unidimensional time series. \n",
    "    Returns (value-mean)/std.\n",
    "    \"\"\"    \n",
    "    \n",
    "    if np.std(S) < 0.01:\n",
    "        return S-np.mean(S)\n",
    "    \n",
    "    return (S-np.mean(S))/np.std(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paa(S_norm, w):\n",
    "    \"\"\"\n",
    "    Piecewise Aggregate Approximation (PAA) of a unidimensional time series (Keogh et. al. (2001)).\n",
    "    S_norm is a normalized time series as a numpy array.\n",
    "    w is the number of segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(S_norm)\n",
    "    S_paa = np.zeros(w)\n",
    "        \n",
    "    if n % w == 0:\n",
    "        # Each segment will automatically have the same number of points\n",
    "        l = n // w # Segment length\n",
    "        for i in range(w):\n",
    "            S_paa[i] = np.mean(S_norm[i*l:(i+1)*l])\n",
    "            \n",
    "    else:\n",
    "        # We have to make sure that each segment gets an equal number of points\n",
    "        points_per_seg = np.zeros(w)\n",
    "        i_last_val = None\n",
    "        i_last_segment = None\n",
    "        \n",
    "        for i in range(n*w):\n",
    "            i_segment = i // n\n",
    "            i_val = i // w            \n",
    "            S_paa[i_segment] += S_norm[i_val]\n",
    "            \n",
    "            # Check that everything is fine \n",
    "            # print(f'n={n}, w={w}, i={i}, i_segment={i_segment}, i_val={i_val}')\n",
    "            \n",
    "        S_paa /= n\n",
    "\n",
    "    return S_paa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_table(a):\n",
    "    \n",
    "    alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "    table = {\n",
    "        2: np.array([-np.inf,  0.00]),\n",
    "        3: np.array([-np.inf, -0.43072, 0.43072]),\n",
    "        4: np.array([-np.inf, -0.67448, 0, 0.67448]),\n",
    "        5: np.array([-np.inf, -0.84162, -0.25334, 0.25334, 0.84162]),\n",
    "        6: np.array([-np.inf, -0.96742, -0.43072, 0, 0.43072, 0.96742]),\n",
    "        7: np.array([-np.inf, -1.06757, -0.56594, -0.18001, 0.18001, 0.56594, 1.06757]),\n",
    "        8: np.array([-np.inf, -1.15034, -0.67448, -0.31863, 0, 0.31863, 0.67448, 1.15034]),\n",
    "        9: np.array([-np.inf, -1.22064, -0.76470, -0.43072, -0.13971, 0.13971, 0.43072, 0.76470, 1.22064]),\n",
    "        10: np.array([-np.inf, -1.28155, -0.84162, -0.52440, -0.25334, 0, 0.25334, 0.52440, 0.84162, 1.28155])\n",
    "    }\n",
    "    \n",
    "    return table[a], alphabet[:a+1]\n",
    "\n",
    "def discretization(S_paa, a):\n",
    "    \n",
    "    breakpoints, alphabet = lookup_table(a)\n",
    "    S_discrete = []\n",
    "    \n",
    "    for val in S_paa:\n",
    "        S_discrete.append(alphabet[np.where(breakpoints<val)[0][-1]])\n",
    "    \n",
    "    return S_discrete, breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_table(a):\n",
    "    \n",
    "    breakpoints, _ = lookup_table(a)\n",
    "    \n",
    "    table = np.zeros((a, a))\n",
    "    \n",
    "    for i in range(table.shape[0]):\n",
    "        for j in range(table.shape[1]):\n",
    "            if abs(i-j) <= 1:\n",
    "                table[i, j] = 0\n",
    "            else:\n",
    "                table[i, j] = breakpoints[max(i, j)] - breakpoints[min(i, j)+1]\n",
    "    \n",
    "    return table\n",
    "\n",
    "def dist(S_discrete_1, S_discrete_2, table):\n",
    "    \n",
    "    distance = np.zeros(len(S_discrete_1))\n",
    "    \n",
    "    for i, (s1, s2) in enumerate(zip(S_discrete_1, S_discrete_2)):\n",
    "        distance[i] = table[ord(s1)-ord('a'), ord(s2)-ord('a')]\n",
    "        \n",
    "        # For verification purposes\n",
    "        # print(f\"i = {i}, s1 = {s1}, s2 = {s2}, value = {table[ord(s1)-ord('a'), ord(s2)-ord('a')]}\")\n",
    "        \n",
    "    return np.sqrt(np.sum(distance**2))\n",
    "\n",
    "def min_dist(S_discrete_1, S_discrete_2, n, w, a):\n",
    "    \"\"\"\n",
    "    Given two symbolic representations of time series S_discrete_1 and S_discrete_2 \n",
    "    as lists of the same length w, computes their distance (Lin et. al. (2007)).\n",
    "    \"\"\"\n",
    "    \n",
    "    prefactor = np.sqrt(n/w)\n",
    "    table = distance_table(a)\n",
    "    \n",
    "    distance = dist(S_discrete_1, S_discrete_2, table)\n",
    "    \n",
    "    return prefactor*distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOP Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOP(S, n, w, a, numerosity_reduction=True, plot=True):\n",
    "    \"\"\"\n",
    "    Implementation of Lin et. al. (2012), a histogram-based representation for time\n",
    "    series data, similar to the “bag of words” approach - called \"bag of patterns\"\n",
    "    \n",
    "        - S: list of  time series as a numpy arrays\n",
    "        - n: size of the sliding window\n",
    "        - w: word length - also length of the extracted patterns\n",
    "        - a: alphabet size\n",
    "    \"\"\"\n",
    "    \n",
    "    S_norm_all = []\n",
    "    S_paa_all = []\n",
    "    S_discrete_all = []\n",
    "    hash_table_all = []\n",
    "    M_all = []\n",
    "        \n",
    "    for s in S:\n",
    "        subsequences = extract_patterns(s, n)\n",
    "\n",
    "        S_norm_list = []\n",
    "        S_paa_list = []\n",
    "        S_discrete_list = []\n",
    "\n",
    "        for seq in subsequences:\n",
    "            S_norm = zscore_norm(seq)\n",
    "            S_paa = paa(S_norm, w)\n",
    "            S_discrete, breakpoints = discretization(S_paa, a)\n",
    "\n",
    "            # Option for numerosity reduction - recommended for slow moving time series\n",
    "            if numerosity_reduction:\n",
    "                # If we have a succession of the same word, we only count it once\n",
    "                if len(S_discrete_list):\n",
    "                    word = ''.join(l for l in S_discrete)\n",
    "                    if word == S_discrete_list[-1]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        S_norm_list.append(S_norm)\n",
    "                        S_paa_list.append(S_paa)\n",
    "                        S_discrete_list.append(''.join(l for l in S_discrete)) \n",
    "                \n",
    "                # If it is the first subsequence, add everything to the arrays\n",
    "                else:\n",
    "                    S_norm_list.append(S_norm)\n",
    "                    S_paa_list.append(S_paa)\n",
    "                    S_discrete_list.append(''.join(l for l in S_discrete)) \n",
    "\n",
    "            # No numerosity reduction\n",
    "            else:\n",
    "                S_norm_list.append(S_norm)\n",
    "                S_paa_list.append(S_paa)\n",
    "                S_discrete_list.append(''.join(l for l in S_discrete))\n",
    "\n",
    "        S_norm = np.concatenate(S_norm_list, axis=None)\n",
    "        S_paa = np.concatenate(S_paa_list, axis=None)\n",
    "        S_discrete = np.concatenate(S_discrete_list, axis=None)\n",
    "\n",
    "        hash_table, M = motifs_table(S_discrete)\n",
    "\n",
    "        S_norm_all.append(S_norm)\n",
    "        S_paa_all.append(S_paa)\n",
    "        S_discrete_all.append(S_discrete)\n",
    "        hash_table_all.append(hash_table)\n",
    "        M_all.append(M)\n",
    "        \n",
    "    if plot:\n",
    "        plot_bop(S, M_all)\n",
    "        \n",
    "    return S_norm_all, S_paa_all, S_discrete_all, hash_table_all, M_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bop_distance(M_all, plot=True):\n",
    "    \"\"\"\n",
    "    The distance is defined as the Euclidian distance between the histograms, i.e. the \n",
    "    Euclidian distance between the pattern's occurrence frequency. \n",
    "    \"\"\"\n",
    "    \n",
    "    distance_matrix = np.zeros((len(M_all), len(M_all)))\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        for j in range(i+1, distance_matrix.shape[1]):\n",
    "            dist = 0\n",
    "            for key in M_all[i].keys():\n",
    "                dist += (M_all[i][key]-M_all[j][key])**2\n",
    "            dist= round(np.sqrt(dist), 2)\n",
    "            distance_matrix[j, i] += dist\n",
    "            \n",
    "    if plot:\n",
    "        f, ax = plt.subplots(figsize=(8, 8))\n",
    "        mask = np.zeros_like(distance_matrix)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        sns.heatmap(distance_matrix, mask=mask, annot=True, fmt='.1f',\n",
    "                    square=True, cbar_kws={'shrink': 0.5}, cmap='Blues', ax=ax)\n",
    "        plt.show()\n",
    "        \n",
    "    return distance_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
